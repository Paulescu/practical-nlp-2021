{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a chatbot using a *seq2seq* model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'formatted_movie_lines.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "--\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "--\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "        print('--')\n",
    "\n",
    "print_lines(FILE, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1. Pre-process the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \n",
    "    PAD_token = 0  # Used for padding short sentences\n",
    "    SOS_token = 1  # Start-of-sentence token\n",
    "    EOS_token = 2  # End-of-sentence token\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {self.PAD_token: \"PAD\", self.SOS_token: \"SOS\", self.EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word in self.word2index:\n",
    "            # this word is already in the vocabulary\n",
    "            self.word2count[word] += 1\n",
    "        else:\n",
    "            # this a new word\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        \n",
    "    def trim(self, min_count):\n",
    "        \"\"\"Removes words from the vocabulary that appear less\n",
    "        than min_count times in the corpus used to build the\n",
    "        vocabulary\"\"\"\n",
    "        if self.trimmed:\n",
    "            # already trimmed, nothing to do\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        for word, count in self.word2count.items():\n",
    "            if count >= min_count:\n",
    "                keep_words.append(word)\n",
    "        \n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        \n",
    "        # generate the vocabulary again, using only 'keep_words'\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {self.PAD_token: \"PAD\", self.SOS_token: \"SOS\", self.EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)\n",
    "            \n",
    "#             print(self.word2count)\n",
    "#             import pdb\n",
    "#             pdb.set_trace()\n",
    "            \n",
    "    def has_word(self, word):\n",
    "        return True if word in self.word2index else False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Num words: {}'.format(self.num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221282/221282 [00:00<00:00, 712108.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original set size:  221282\n",
      "After normalization:  221282\n",
      "After filtering:   64271\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def load_pairs(file, n=None):\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = []\n",
    "    for line in tqdm(lines):\n",
    "        pair = line.split('\\t')\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "pairs = load_pairs(FILE)\n",
    "print('Original set size: ', len(pairs))\n",
    "\n",
    "import unicodedata\n",
    "def unicode2Ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalize_pairs(pairs):\n",
    "    def normalize_string(s):\n",
    "        s = unicode2Ascii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "        return s\n",
    "    \n",
    "    pairs = [[normalize_string(p[0]), normalize_string(p[1])] for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "pairs = normalize_pairs(pairs)\n",
    "print('After normalization: ', len(pairs))\n",
    "\n",
    "def filter_pairs_by_length(pairs, max_length: int = 999999):\n",
    "    \n",
    "    def is_pair_short_enough(p):\n",
    "        return (len(p[0].split(' ')) < max_length) and (len(p[1].split(' ')) < max_length)\n",
    "    \n",
    "    return [pair for pair in pairs if is_pair_short_enough(pair)]\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "pairs = filter_pairs_by_length(pairs, max_length=MAX_LENGTH)\n",
    "print('After filtering:  ', len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words: 18008\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "def generate_vocabulary(pairs):\n",
    "    \n",
    "    vocab = Vocab()\n",
    "    for pair in pairs:\n",
    "        vocab.add_sentence(pair[0])\n",
    "        vocab.add_sentence(pair[1])\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "MIN_WORD_COUNT = 3\n",
    "vocab = generate_vocabulary(pairs)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Num words: 7826\n",
      "Trimmed from 64271 pairs to 53165 pairs, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "def filter_pairs_with_rare_words(pairs, vocab, min_count):\n",
    "    vocab.trim(min_count)\n",
    "    print(vocab)\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        keep_pair = True\n",
    "        \n",
    "        # check if all words from 1st sentence are NOT rare\n",
    "        for word in pair[0].split(' '):\n",
    "            if not vocab.has_word(word):\n",
    "                keep_pair = False\n",
    "                break\n",
    "\n",
    "        if not keep_pair:\n",
    "            continue\n",
    "        \n",
    "        for word in pair[1].split(' '):\n",
    "            if not vocab.has_word(word):\n",
    "                keep_pair = False\n",
    "                break\n",
    "        \n",
    "        if keep_pair:\n",
    "            keep_pairs.append(pair)\n",
    "        \n",
    "    print('Trimmed from {} pairs to {} pairs, {:.4f} of total'.format(\n",
    "        len(pairs), len(keep_pairs), len(keep_pairs)/len(pairs)))\n",
    "\n",
    "    return keep_pairs, vocab\n",
    "\n",
    "pairs, vocab = filter_pairs_with_rare_words(pairs, vocab, MIN_WORD_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'where ?']\n",
      "[[3, 4], [5, 6]]\n",
      "\n",
      "\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "[[7, 8, 9, 10, 4, 11, 12, 13], [7, 14, 15, 4]]\n",
      "\n",
      "\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "[[16, 4], [17, 18, 19, 20, 21, 22, 23, 6]]\n",
      "\n",
      "\n",
      "['have fun tonight ?', 'tons']\n",
      "[[8, 31, 22, 6], [32]]\n",
      "\n",
      "\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "[[33, 34, 4, 4, 4], [35, 36, 37, 38, 7, 39, 40, 41, 4]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset and Dataloader\n",
    "def tokenize_pairs(pairs, vocab):\n",
    "    tokenized_pairs = []\n",
    "    for pair in pairs:\n",
    "        tokenized_sentence_0 = []\n",
    "        tokenized_sentence_1 = []\n",
    "        for word in pair[0].split(' '):\n",
    "            tokenized_sentence_0.append(vocab.word2index[word])\n",
    "        for word in pair[1].split(' '):\n",
    "            tokenized_sentence_1.append(vocab.word2index[word])\n",
    "        tokenized_pairs.append([tokenized_sentence_0, tokenized_sentence_1])\n",
    "    \n",
    "    return tokenized_pairs\n",
    "\n",
    "tokenized_pairs = tokenize_pairs(pairs, vocab)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(pairs[i])\n",
    "    print(tokenized_pairs[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
