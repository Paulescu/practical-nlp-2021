{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10: Fine-tunning BERT for sentiment analysis using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Paulescu/practical-nlp-2021/blob/main/notebooks/1_fine_tune_bert_for_sentiment_analysis.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings:\n",
    "\n",
    "At the end of this lesson you will know:\n",
    "\n",
    "- How to leverage the power of massive pre-trained models for you particular ML problem.\n",
    "\n",
    "- How to use the HuggingFace Transformers API to quickly build a training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-24 16:47:48--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
      "\n",
      "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M   865KB/s    in 48s     \n",
      "\n",
      "2020-11-24 16:48:36 (1.67 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data into Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts:  25000\n",
      "test_texts:  25000\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir: str) -> Tuple[List[str], List[str]]:\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in ['pos', 'neg']:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "train_texts_, train_labels_ = read_imdb_split('aclImdb/train')\n",
    "print('train_texts: ', len(train_texts_))\n",
    "\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')\n",
    "print('test_texts: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split data into train/validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = \\\n",
    "    train_test_split(train_texts_, train_labels_, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text tokenization\n",
    "\n",
    "A tokenizer maps a string (sentence) to a list of integers (token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In Transformers v4.0.0, the default path to cache downloaded models changed from '~/.cache/torch/transformers' to '~/.cache/huggingface/transformers'. Since you don't seem to have overridden and '~/.cache/torch/transformers' is a directory that exists, we're moving it to '~/.cache/huggingface/transformers' to avoid redownloading models you have already in the cache. You should only see this message once.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encodings.input_ids[0]))\n",
    "print(len(train_encodings.input_ids[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [101, 3100, 1010, 2061, 1045, 2131, 2009, 1012, 2057, 1005, 2128, 4011, 2000, 2022, 14603, 1012, 1996, 2801, 2038, 2042, 8461, 1012, 1037, 2611, 2003, 2725, 2014, 3611, 1998, 2635, 7760, 1997, 2009, 1012, 2655, 2033, 2058, 1996, 5213, 1011, 2600, 6907, 2021, 1045, 2655, 2005, 1996, 13216, 17555, 1997, 2019, 2552, 2077, 1045, 2064, 2991, 2005, 2023, 1012, 2021, 2123, 1005, 1056, 5987, 2033, 2000, 3422, 1037, 3730, 1011, 22555, 1998, 2468, 14603, 2008, 2016, 2003, 1005, 2725, 2014, 2269, 1005, 1012, 1012, 1012, 1045, 2812, 8440, 1005, 1056, 2008, 4680, 2468, 1037, 2978, 16999, 1999, 1996, 4639, 2143, 3068, 2525, 29543, 2094, 2007, 1005, 9040, 1010, 1998, 16709, 20100, 1005, 22555, 1012, 1012, 1012, 5469, 3475, 1005, 1056, 2054, 2115, 2568, 2064, 7966, 2017, 2046, 8929, 1012, 2009, 2003, 2054, 2941, 6526, 1999, 2143, 1012, 2023, 2003, 2073, 2771, 17339, 11896, 1999, 10367, 1053, 1012, 4654, 7913, 26725, 4150, 10256, 2043, 2009, 4150, 1037, 5454, 2115, 2219, 6172, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "\n",
    "for key, value in train_encodings.items():\n",
    "    print(key, value[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of text tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \t I loved the movie\n",
      "Tokens: \t ['[CLS]', 'i', 'loved', 'the', 'movie', '[SEP]']\n",
      "Token ids: \t [101, 1045, 3866, 1996, 3185, 102]\n",
      "\n",
      "Original text: \t I couldn't understand the movie\n",
      "Tokens: \t ['[CLS]', 'i', 'couldn', \"'\", 't', 'understand', 'the', 'movie', '[SEP]']\n",
      "Token ids: \t [101, 1045, 2481, 1005, 1056, 3305, 1996, 3185, 102]\n",
      "\n",
      "Original text: \t I have seen this movie 23 times!!!\n",
      "Tokens: \t ['[CLS]', 'i', 'have', 'seen', 'this', 'movie', '23', 'times', '!', '!', '!', '[SEP]']\n",
      "Token ids: \t [101, 1045, 2031, 2464, 2023, 3185, 2603, 2335, 999, 999, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "sentence = \"I loved the movie\"\n",
    "mock_encodings = tokenizer(sentence, truncation=True, padding=True)\n",
    "print('Original text: \\t', sentence)\n",
    "print('Tokens: \\t', mock_encodings.tokens())\n",
    "print('Token ids: \\t', mock_encodings.input_ids)\n",
    "\n",
    "# example 2\n",
    "sentence = \"I couldn't understand the movie\"\n",
    "mock_encodings = tokenizer(sentence, truncation=True, padding=True)\n",
    "print('\\nOriginal text: \\t', sentence)\n",
    "print('Tokens: \\t', mock_encodings.tokens())\n",
    "print('Token ids: \\t', mock_encodings.input_ids)\n",
    "\n",
    "# example 3\n",
    "sentence = \"I have seen this movie 23 times!!!\"\n",
    "mock_encodings = tokenizer(sentence, truncation=True, padding=True)\n",
    "print('\\nOriginal text: \\t', sentence)\n",
    "print('Tokens: \\t', mock_encodings.tokens())\n",
    "print('Token ids: \\t', mock_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a PyTorch Dataset object\n",
    "\n",
    "### How to create a custom dataset in PyTorch?\n",
    "\n",
    "Steps:\n",
    "- Define a new class (e.g. MyDataset) that extends torch.utils.data.Dataset.\n",
    "- Overwrite __getitem__(idx) method.\n",
    "- Overwrite __len__() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, train_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorboard to visualize metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b123bc86cfecf1c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b123bc86cfecf1c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "global_train_step = 0\n",
    "\n",
    "# Setup logging to Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_val_epoch(\n",
    "    # model,\n",
    "    batch_loader: DataLoader,\n",
    "    epoch: int,\n",
    "    is_train: bool=True\n",
    "    ):\n",
    "    \n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_predictions = 0\n",
    "    epoch_correct_predictions = 0\n",
    "    for batch in batch_loader:\n",
    "        \n",
    "        # forward pass\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        _, predictions = torch.max(outputs[1], 1)\n",
    "\n",
    "        if is_train:\n",
    "            # backward pass\n",
    "            optim.zero_grad()    \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        # batch stats\n",
    "        batch_size = input_ids.shape[0]\n",
    "        batch_correct_predictions = predictions.eq(labels.data).sum().item()\n",
    "        batch_accuracy = batch_correct_predictions/batch_size\n",
    "        batch_loss = loss.item()\n",
    "\n",
    "        # epoch stats\n",
    "        epoch_correct_predictions += batch_correct_predictions\n",
    "        epoch_predictions += batch_size     \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # log batch metrics, only in train mode.\n",
    "        # The purpose is to verify the loss is actually going down as we traverse\n",
    "        # the whole train set.\n",
    "        if is_train:\n",
    "            global global_train_step\n",
    "            global_train_step += batch_size\n",
    "            writer.add_scalar('training_batch_loss',\n",
    "                              batch_loss,\n",
    "                              global_train_step)\n",
    "            writer.add_scalar('training_batch_accuracy',\n",
    "                              batch_accuracy,\n",
    "                              global_train_step)\n",
    "      \n",
    "    # epoch loss and accuracy\n",
    "    epoch_loss = epoch_loss / epoch_predictions \n",
    "    epoch_accuracy = epoch_correct_predictions / epoch_predictions\n",
    "\n",
    "    # log epoch metrics, both in train and validation mode.\n",
    "    epoch_loss_metric_name = 'training_epoch_loss' if is_train \\\n",
    "        else 'validation_epoch_loss'\n",
    "    epoch_accuracy_metric_name = 'training_epoch_accuracy' if is_train \\\n",
    "        else 'validation_epoch_accuracy'\n",
    "    writer.add_scalar(epoch_loss_metric_name, epoch_loss, epoch)\n",
    "    writer.add_scalar(epoch_accuracy_metric_name, epoch_accuracy, epoch)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # train\n",
    "    train_val_epoch(train_loader, epoch, is_train=True)\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        train_val_epoch(val_loader, epoch, is_train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
