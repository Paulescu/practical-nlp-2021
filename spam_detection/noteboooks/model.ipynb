{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://archive.ics.uci.edu/ml/assets/logo.gif\" align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-word model with learned embeddings from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Paulescu/practical-nlp-2021/blob/main/0_sentiment_analysis/0_bag_of_words_with_learned_embeddings.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_COMPUTER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0. Set up Python environment\n",
    "This is necessary when you want to run the notebook in a different computer, like Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Python environment...\n"
     ]
    }
   ],
   "source": [
    "if not LOCAL_COMPUTER:\n",
    "    print('Setting up Python environment...')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1. Download data and split into train, validation and test\n",
    "\n",
    "The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-10 18:51:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip.2’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   271KB/s    in 0.7s    \n",
      "\n",
      "2020-12-10 18:51:56 (271 KB/s) - ‘smsspamcollection.zip.2’ saved [203415/203415]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!tar -xf smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numeric column for the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_int\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX_TO_LABEL = {\n",
    "    0: 'ham',\n",
    "    1: 'spam',\n",
    "}\n",
    "\n",
    "LABEL_TO_IDX = {\n",
    "    'ham': 0,\n",
    "    'spam': 1,\n",
    "}\n",
    "\n",
    "data['label_int'] = data['label'].apply(lambda x: LABEL_TO_IDX[x])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into files `train.csv` , `validation.csv`, `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  3565\n",
      "validation_data:  892\n",
      "test_data:  1115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.20, random_state=123,)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.20, random_state=123)\n",
    "\n",
    "print('train_data: ', len(train_data))\n",
    "print('validation_data: ', len(validation_data))\n",
    "print('test_data: ', len(test_data))\n",
    "\n",
    "train_data[['label_int', 'text']].to_csv('train.csv', index=False, header=False)\n",
    "validation_data[['label_int', 'text']].to_csv('validation.csv', index=False, header=False)\n",
    "test_data[['label_int', 'text']].to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2. Set up PyTorch `DataLoader`s to tokenize text feature and feed it into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Field`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.54.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "import spacy\n",
    "\n",
    "spacy_eng = spacy.load('en')\n",
    "\n",
    "def tokenizer_fn(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer_fn, lower=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `Dataset`s for train, validation and test\n",
    "\n",
    "`TabularDataset` provides a convenient method to generate PyTorch `Dataset` objects from csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "fields = [('label', LABEL), ('text', TEXT)]\n",
    "\n",
    "train, validation, test = TabularDataset.splits(\n",
    "    path='',\n",
    "    train='train.csv',\n",
    "    validation='validation.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['mom', 'wants', 'to', 'know', 'where', 'you', 'at']\n",
      "label:  0\n",
      "---\n",
      "text:  ['boy', ';', 'i', 'love', 'u', 'grl', ':', 'hogolo', 'boy', ':', 'gold', 'chain', 'kodstini', 'grl', ':', 'agalla', 'boy', ':', 'necklace', 'madstini', 'grl', ':', 'agalla', 'boy', ':', 'hogli', '1', 'mutai', 'eerulli', 'kodthini', '!', 'grl', ':', 'i', 'love', 'u', 'kano;-', ')']\n",
      "label:  0\n",
      "---\n",
      "text:  ['its', 'on', 'in', 'engalnd', '!', 'but', 'telly', 'has', 'decided', 'it', 'wo', \"n't\", 'let', 'me', 'watch', 'it', 'and', 'mia', 'and', 'elliot', 'were', 'kissing', '!', 'damn', 'it', '!']\n",
      "label:  0\n",
      "---\n",
      "text:  ['your', 'gon', 'na', 'have', 'to', 'pick', 'up', 'a', '$', '1', 'burger', 'for', 'yourself', 'on', 'your', 'way', 'home', '.', 'i', 'ca', \"n't\", 'even', 'move', '.', 'pain', 'is', 'killing', 'me', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['no', 'no:)this', 'is', 'kallis', 'home', 'ground.amla', 'home', 'town', 'is', 'durban', ':', ')']\n",
      "label:  0\n",
      "---\n",
      "text:  ['i', 'am', 'seeking', 'a', 'lady', 'in', 'the', 'street', 'and', 'a', 'freak', 'in', 'the', 'sheets', '.', 'is', 'that', 'you', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['yeah', ',', 'in', 'fact', 'he', 'just', 'asked', 'if', 'we', 'needed', 'anything', 'like', 'an', 'hour', 'ago', '.', 'when', 'and', 'how', 'much', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['where', 'r', 'we', 'meeting', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem', '!', '!', '!']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ok', 'try', 'to', 'do', 'week', 'end', 'course', 'in', 'coimbatore', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['not', 'a', 'lot', 'has', 'happened', 'here', '.', 'feels', 'very', 'quiet', '.', 'beth', 'is', 'at', 'her', 'aunts', 'and', 'charlie', 'is', 'working', 'lots', '.', 'just', 'me', 'and', 'helen', 'in', 'at', 'the', 'mo', '.', 'how', 'have', 'you', 'been', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['i', 'hope', 'your', 'alright', 'babe', '?', 'i', 'worry', 'that', 'you', 'might', 'have', 'felt', 'a', 'bit', 'desparate', 'when', 'you', 'learned', 'the', 'job', 'was', 'a', 'fake', '?', 'i', 'am', 'here', 'waiting', 'when', 'you', 'come', 'back', ',', 'my', 'love']\n",
      "label:  0\n",
      "---\n",
      "text:  ['what', 'is', 'your', 'account', 'number', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['hahaha', '..', 'use', 'your', 'brain', 'dear']\n",
      "label:  0\n",
      "---\n",
      "text:  ['otherwise', 'had', 'part', 'time', 'job', 'na', '-', 'tuition', '..']\n",
      "label:  0\n",
      "---\n",
      "text:  ['so', 'wat', \"'s\", 'da', 'decision', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ups', 'which', 'is', '3days', 'also', ',', 'and', 'the', 'shipping', 'company', 'that', 'takes', '2wks', '.', 'the', 'other', 'way', 'is', 'usps', 'which', 'takes', 'a', 'week', 'but', 'when', 'it', 'gets', 'to', 'lag', 'you', 'may', 'have', 'to', 'bribe', 'nipost', 'to', 'get', 'your', 'stuff', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['horrible', 'gal', '.', 'me', 'in', 'sch', 'doing', 'some', 'stuff', '.', 'how', 'come', 'u', 'got', 'mc', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['urgent', '!', '!', 'your', '4', '*', 'costa', 'del', 'sol', 'holiday', 'or', '£', '5000', 'await', 'collection', '.', 'call', '09050090044', 'now', 'toclaim', '.', 'sae', ',', 'tc', 's', ',', 'pobox334', ',', 'stockport', ',', 'sk38xh', ',', 'cost£1.50', '/', 'pm', ',', 'max10mins']\n",
      "label:  1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('text: ', train[i].text)\n",
    "    print('label: ', train[i].label)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary using the train set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3337\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size=10000, min_freq=2)\n",
    "\n",
    "# we will need this later\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print('Vocabulary size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataloader`s for train, validation and test\n",
    "\n",
    "In `torchtext` data loaders are called `BucketIterator`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/spam_detection/.venv/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iter, validation_iter, test_iter = BucketIterator.splits(\n",
    "    (train, validation, test),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3. Define the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add diagram here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.global_avg_pooling = lambda x: torch.mean(x, dim=-2)\n",
    "        self.fc1 = nn.Linear(embedding_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "model = Model(vocab_size, EMBEDDING_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 52119), started 1:41:26 ago. (Use '!kill 52119' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-96fcb66829ceb2ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-96fcb66829ceb2ce\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b967c6471544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d-%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bag_of_words_embeddings_from_scratch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./runs/{model_name}/{now}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup logging to Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "MODEL_NAME = 'bag_of_words_embeddings_from_scratch'\n",
    "log_file = f'./runs/{MODEL_NAME}/{now}'\n",
    "writer = SummaryWriter(log_file)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "N_EPOCHS = 150\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_size = 0\n",
    "    running_accuracy = 0.0\n",
    "    for batch in tqdm(train_iter):\n",
    "        \n",
    "        # forward pass to compute the batch loss\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "            \n",
    "        # backward pass to update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute train metrics\n",
    "        running_loss += loss.data * x.size(0)\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        running_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        train_size += x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_accuracy = running_accuracy / train_size\n",
    "    \n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    val_size = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_iter:\n",
    "            x = batch.text\n",
    "            y = batch.label.long()\n",
    "            predictions = model(x)\n",
    "            loss = criterion(predictions, y)\n",
    "            \n",
    "            # compute validation metrics\n",
    "            val_loss += loss.data * x.size(0)\n",
    "            _, predicted_classes = torch.max(predictions, 1)\n",
    "            val_accuracy += predicted_classes.eq(y.data).sum().item()           \n",
    "            val_size += x.size(0)\n",
    "            \n",
    "        val_loss /= val_size\n",
    "        val_accuracy /= val_size\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Loss \\t Train: {:.4f} \\t Validation: {:.4f}'.format(epoch_loss, val_loss))\n",
    "        print('Acc: \\t Train: {:.4f} \\t Validation: {:.4f}'.format(epoch_accuracy, val_accuracy))\n",
    "\n",
    "    # log metrics to tensorboard\n",
    "    writer.add_scalars('Loss', {'train': epoch_loss, 'validation': val_loss}, epoch + 1)\n",
    "    writer.add_scalars('Accuracy', {'train': epoch_accuracy, 'validation': val_accuracy}, epoch + 1)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = 0.0\n",
    "test_size = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        # forward pass\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        test_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        test_size += x.size(0)\n",
    "\n",
    "test_accuracy /= test_size\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with the model\n",
    "Pay attention how pre-processing and post-processing are necessary to be able to use the model at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'This is your friend Carl. Come to the Casino and get a discount!',\n",
    "    'This is your friend Carl, do you want to meet later?',\n",
    "    'Would you be interested in buying a car for nothing?',\n",
    "    'Send your card details today and get a prize!',\n",
    "    'I won two tickets to the show, do you want to come with me?',\n",
    "    'I won two tickets to the show, just send an SMS to this number and get them',\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    # pre-process text into integer tokens\n",
    "    model_input = [TEXT.vocab.stoi[token] for token in tokenizer_fn(s)]\n",
    "    # add 0-dimension\n",
    "    model_input = torch.LongTensor(model_input).unsqueeze(0)\n",
    "    \n",
    "    # run model prediction\n",
    "    predictions = model(model_input)\n",
    "    \n",
    "    # post-processing\n",
    "    _, predicted_class = torch.max(predictions, 1)\n",
    "    predicted_class = predicted_class.item()\n",
    "    predicted_class_str = IDX_TO_LABEL[predicted_class]\n",
    "    \n",
    "    # print\n",
    "    print(s)\n",
    "    print(predicted_class_str)\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Visualize the learned word embeddings with the [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if name == 'embed.weight':\n",
    "        embeddings = parameter\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "vocab = TEXT.vocab.itos\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index in [0, 1]:\n",
    "        # skip 0, it's the unknown token.\n",
    "        # skip 1, it's the padding token.\n",
    "        continue\n",
    "        \n",
    "    vec = embeddings[index, :] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files to your local computer (in case you are running this notebook in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
