{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to painlessly transform an NLP model in Jupyter to a production API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://archive.ics.uci.edu/ml/assets/logo.gif\" align='left' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-words model with word embeddings learned from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Paulescu/practical-nlp-2021/blob/main/spam_detection/noteboooks/model.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required setup if you run the notebook in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python setup skiped.\n"
     ]
    }
   ],
   "source": [
    "# # you need to paste the URL of your Github repo here if you want to run this notebook in Google Colab.\n",
    "# URL_GITHUB_REPO = 'https://github.com/Paulescu/practical-nlp-2021'\n",
    "\n",
    "# # a hacky way to check if the current notebook is running in Google Colab.\n",
    "# if 'google.colab' in str(get_ipython()):\n",
    "#     # we are running notebook in Colab\n",
    "#     !git clone $URL_GITHUB_REPO\n",
    "#     !cd .. && python setup.py develop\n",
    "# else:\n",
    "#     print('Python setup skiped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Download data and split into train, validation and test\n",
    "\n",
    "The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-10 18:51:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip.2’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   271KB/s    in 0.7s    \n",
      "\n",
      "2020-12-10 18:51:56 (271 KB/s) - ‘smsspamcollection.zip.2’ saved [203415/203415]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!tar -xf smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "data.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add numeric column for the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_int\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDX_TO_LABEL = {\n",
    "    0: 'ham',\n",
    "    1: 'spam',\n",
    "}\n",
    "\n",
    "LABEL_TO_IDX = {\n",
    "    'ham': 0,\n",
    "    'spam': 1,\n",
    "}\n",
    "\n",
    "data['label_int'] = data['label'].apply(lambda x: LABEL_TO_IDX[x])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into files `train.csv` , `validation.csv`, `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  3565\n",
      "validation_data:  892\n",
      "test_data:  1115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.20, random_state=123,)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.20, random_state=123)\n",
    "\n",
    "print('train_data: ', len(train_data))\n",
    "print('validation_data: ', len(validation_data))\n",
    "print('test_data: ', len(test_data))\n",
    "\n",
    "train_data[['label_int', 'text']].to_csv('train.csv', index=False, header=False)\n",
    "validation_data[['label_int', 'text']].to_csv('validation.csv', index=False, header=False)\n",
    "test_data[['label_int', 'text']].to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Define PyTorch `DataLoader`s for train, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "try:\n",
    "    spacy_eng = spacy.load('en')\n",
    "except:\n",
    "    print('Downloading spacy resources for English')\n",
    "    !python -m spacy download en\n",
    "    spacy_eng = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3337\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "def tokenizer_fn(text):\n",
    "    # Input: \"Come to see me!\"\n",
    "    # Output: \"['Come', 'to', 'see', 'me', '!']\"\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "# Create PyTorch Datasets from train.csv, validation.csv, test.csv\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer_fn, lower=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, validation, test = TabularDataset.splits(\n",
    "    path='',\n",
    "    train='train.csv',\n",
    "    validation='validation.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=[('label', LABEL), ('text', TEXT)],\n",
    ")\n",
    "\n",
    "# Build the vocabulary using the train dataset\n",
    "TEXT.build_vocab(train, max_size=10000, min_freq=2)\n",
    "vocab_size = len(TEXT.vocab) # we will need this later\n",
    "print('Vocabulary size: ', vocab_size)\n",
    "\n",
    "# Create PyTorch DataLoaders for train, validation, test\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "train_iter, validation_iter, test_iter = BucketIterator.splits(\n",
    "    (train, validation, test),\n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    device=DEVICE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check output from `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 182,    2,  185,   36, 1611],\n",
      "        [  50, 3189, 1897,    0, 1754],\n",
      "        [  68,  303,  121,    9,  597],\n",
      "        [ 140,  110,   40,  290,    8],\n",
      "        [   0,    0, 1169,    0,    2],\n",
      "        [ 150,   12,  174,   86,    8],\n",
      "        [1591,  123,   48,   92,    2],\n",
      "        [  63,  800,   15, 1541,    8],\n",
      "        [  37,   50,   21,   24,    0],\n",
      "        [  96,   15,  867,    0,    8],\n",
      "        [ 422,  101,  136,    0, 1420],\n",
      "        [   0,   27,    4,   97,    8],\n",
      "        [  54,   82,   28,   16,  160],\n",
      "        [  17,  287,   20, 2331,  287],\n",
      "        [ 150,   99,  174,   32,    8],\n",
      "        [   3,   42,  279,  693,   11],\n",
      "        [ 182,   86,  296,  234,    0],\n",
      "        [   3,  374,  587,   24,  688],\n",
      "        [  74,   48,  828,   58,    8],\n",
      "        [ 270,  118,   14,    0,   20],\n",
      "        [   0,   25,   75,    0,  313],\n",
      "        [   0,  126,  263,  252,    8],\n",
      "        [  40,  110,  901,  175,    2],\n",
      "        [  27,    4,   51,  165, 2442],\n",
      "        [  81,   14,    7,    0, 2334],\n",
      "        [  19,  721,   14,    7,    0],\n",
      "        [   0,    0,    0,  474,  122],\n",
      "        [   3,   76,  106,  424,   17],\n",
      "        [   3,  196,  908,  175,    2],\n",
      "        [  99,  424,  509,  693,   11],\n",
      "        [ 612,  296,   16, 2625,    0],\n",
      "        [  17,  287,   20, 2331,  287],\n",
      "        [  63,   54,   12, 1699,    8],\n",
      "        [ 142,   68,   52,  907,   20],\n",
      "        [  22,   16,  669,  801,   20],\n",
      "        [ 139,    3,   25,    0,  330],\n",
      "        [   0,  169,   35,  656,   10],\n",
      "        [ 572,    0,    0,  829,   20],\n",
      "        [2176, 2368,   81,    0,    1],\n",
      "        [  74,  197,  252,    8,    1],\n",
      "        [   0,   22, 3058,   20,    1],\n",
      "        [  39,  101,  201,    2,    1],\n",
      "        [  24,   29, 2933, 3055,    1],\n",
      "        [  68,   52,  907,   11,    1],\n",
      "        [  52, 2000,    0,    2,    1],\n",
      "        [   0,    0,    0, 2605,    1],\n",
      "        [1301,    0,  500,    0,    1],\n",
      "        [ 307, 1066,  397,   11,    1],\n",
      "        [ 365,   45,    0,    2,    1],\n",
      "        [ 109,   15,  250,  229,    1],\n",
      "        [ 342,    2,   38,   11,    1],\n",
      "        [ 296, 1356,   28,    0,    1],\n",
      "        [ 211,    0,    0,  203,    1],\n",
      "        [ 336, 1039,  437,   11,    1],\n",
      "        [   0,    0, 2696,    2,    1],\n",
      "        [ 403,  500,  100,   11,    1],\n",
      "        [1531,   24, 2979,  925,    1],\n",
      "        [  24, 1020, 3259,    0,    1],\n",
      "        [ 170,   44,    9, 1183,    1],\n",
      "        [  24, 1020, 3259,    0,    1],\n",
      "        [ 121,  140,    4,  908,    1],\n",
      "        [1489,   16,  312,   11,    1],\n",
      "        [   0,  148, 2062,    2,    1],\n",
      "        [1578,   39,    9,    0,    1],\n",
      "        [  50,  288,  107,  101,    1],\n",
      "        [ 220,    0,   66,    2,    1],\n",
      "        [  13,  399,  136,    0,    1],\n",
      "        [  38,    7,   35,    8,    1],\n",
      "        [  15,  114,  335, 1146,    1],\n",
      "        [ 717,  645,    0,    0,    1],\n",
      "        [   3,   42,  279,   11,    1],\n",
      "        [   0,    0,  688,    2,    1],\n",
      "        [ 381,  394, 1791,   11,    1],\n",
      "        [ 166,    0,    2,    1,    1],\n",
      "        [ 655,  100,   11,    1,    1],\n",
      "        [  52,  659, 3218,    1,    1],\n",
      "        [  68,  100,   11,    1,    1],\n",
      "        [  68,  100,    2,    1,    1],\n",
      "        [3167,   35, 1608,    1,    1],\n",
      "        [ 385,   12,   10,    1,    1],\n",
      "        [ 197,  340,   11,    1,    1],\n",
      "        [ 307,   68,   11,    1,    1],\n",
      "        [  68,  100,   11,    1,    1],\n",
      "        [  68,   38,   11,    1,    1],\n",
      "        [  68,  100,   11,    1,    1],\n",
      "        [2696,    0,    0,    1,    1],\n",
      "        [3167,   35, 1608,    1,    1],\n",
      "        [ 307,   68,   11,    1,    1],\n",
      "        [  58,   78,    8,    1,    1],\n",
      "        [  68,  458,   11,    1,    1],\n",
      "        [ 612,    0,    0,    1,    1],\n",
      "        [  68,  100,    2,    1,    1],\n",
      "        [ 170,  651,  217,    1,    1],\n",
      "        [  68,  458,   11,    1,    1],\n",
      "        [   0,    0,   10,    1,    1],\n",
      "        [ 375,  634,    8,    1,    1],\n",
      "        [  16,  118,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [   0,    0,    1,    1,    1],\n",
      "        [   0,    0,    1,    1,    1],\n",
      "        [ 182,  287,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [  68,    2,    1,    1,    1],\n",
      "        [ 655,   11,    1,    1,    1],\n",
      "        [ 307,   11,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [   0,    0,    1,    1,    1],\n",
      "        [  68,    2,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [   0,  177,    1,    1,    1],\n",
      "        [ 642,   11,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [   0,    8,    1,    1,    1],\n",
      "        [2605,    0,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [  68,   20,    1,    1,    1],\n",
      "        [ 565,  565,    1,    1,    1],\n",
      "        [  68,   11,    1,    1,    1],\n",
      "        [ 140,  847,    1,    1,    1],\n",
      "        [ 642,    1,    1,    1,    1],\n",
      "        [  68,    1,    1,    1,    1],\n",
      "        [   0,    1,    1,    1,    1],\n",
      "        [ 642,    1,    1,    1,    1],\n",
      "        [ 642,    1,    1,    1,    1],\n",
      "        [ 249,    1,    1,    1,    1],\n",
      "        [1772,    1,    1,    1,    1],\n",
      "        [  68,    1,    1,    1,    1]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "train_input = next(iter(train_iter))\n",
    "\n",
    "print(train_input.text)\n",
    "print(train_input.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['mom', 'wants', 'to', 'know', 'where', 'you', 'at']\n",
      "label:  0\n",
      "---\n",
      "text:  ['boy', ';', 'i', 'love', 'u', 'grl', ':', 'hogolo', 'boy', ':', 'gold', 'chain', 'kodstini', 'grl', ':', 'agalla', 'boy', ':', 'necklace', 'madstini', 'grl', ':', 'agalla', 'boy', ':', 'hogli', '1', 'mutai', 'eerulli', 'kodthini', '!', 'grl', ':', 'i', 'love', 'u', 'kano;-', ')']\n",
      "label:  0\n",
      "---\n",
      "text:  ['its', 'on', 'in', 'engalnd', '!', 'but', 'telly', 'has', 'decided', 'it', 'wo', \"n't\", 'let', 'me', 'watch', 'it', 'and', 'mia', 'and', 'elliot', 'were', 'kissing', '!', 'damn', 'it', '!']\n",
      "label:  0\n",
      "---\n",
      "text:  ['your', 'gon', 'na', 'have', 'to', 'pick', 'up', 'a', '$', '1', 'burger', 'for', 'yourself', 'on', 'your', 'way', 'home', '.', 'i', 'ca', \"n't\", 'even', 'move', '.', 'pain', 'is', 'killing', 'me', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['no', 'no:)this', 'is', 'kallis', 'home', 'ground.amla', 'home', 'town', 'is', 'durban', ':', ')']\n",
      "label:  0\n",
      "---\n",
      "text:  ['i', 'am', 'seeking', 'a', 'lady', 'in', 'the', 'street', 'and', 'a', 'freak', 'in', 'the', 'sheets', '.', 'is', 'that', 'you', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['yeah', ',', 'in', 'fact', 'he', 'just', 'asked', 'if', 'we', 'needed', 'anything', 'like', 'an', 'hour', 'ago', '.', 'when', 'and', 'how', 'much', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['where', 'r', 'we', 'meeting', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['wat', 'makes', 'some', 'people', 'dearer', 'is', 'not', 'just', 'de', 'happiness', 'dat', 'u', 'feel', 'when', 'u', 'meet', 'them', 'but', 'de', 'pain', 'u', 'feel', 'when', 'u', 'miss', 'dem', '!', '!', '!']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ok', 'try', 'to', 'do', 'week', 'end', 'course', 'in', 'coimbatore', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['not', 'a', 'lot', 'has', 'happened', 'here', '.', 'feels', 'very', 'quiet', '.', 'beth', 'is', 'at', 'her', 'aunts', 'and', 'charlie', 'is', 'working', 'lots', '.', 'just', 'me', 'and', 'helen', 'in', 'at', 'the', 'mo', '.', 'how', 'have', 'you', 'been', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['i', 'hope', 'your', 'alright', 'babe', '?', 'i', 'worry', 'that', 'you', 'might', 'have', 'felt', 'a', 'bit', 'desparate', 'when', 'you', 'learned', 'the', 'job', 'was', 'a', 'fake', '?', 'i', 'am', 'here', 'waiting', 'when', 'you', 'come', 'back', ',', 'my', 'love']\n",
      "label:  0\n",
      "---\n",
      "text:  ['what', 'is', 'your', 'account', 'number', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['hahaha', '..', 'use', 'your', 'brain', 'dear']\n",
      "label:  0\n",
      "---\n",
      "text:  ['otherwise', 'had', 'part', 'time', 'job', 'na', '-', 'tuition', '..']\n",
      "label:  0\n",
      "---\n",
      "text:  ['so', 'wat', \"'s\", 'da', 'decision', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['ups', 'which', 'is', '3days', 'also', ',', 'and', 'the', 'shipping', 'company', 'that', 'takes', '2wks', '.', 'the', 'other', 'way', 'is', 'usps', 'which', 'takes', 'a', 'week', 'but', 'when', 'it', 'gets', 'to', 'lag', 'you', 'may', 'have', 'to', 'bribe', 'nipost', 'to', 'get', 'your', 'stuff', '.']\n",
      "label:  0\n",
      "---\n",
      "text:  ['horrible', 'gal', '.', 'me', 'in', 'sch', 'doing', 'some', 'stuff', '.', 'how', 'come', 'u', 'got', 'mc', '?']\n",
      "label:  0\n",
      "---\n",
      "text:  ['urgent', '!', '!', 'your', '4', '*', 'costa', 'del', 'sol', 'holiday', 'or', '£', '5000', 'await', 'collection', '.', 'call', '09050090044', 'now', 'toclaim', '.', 'sae', ',', 'tc', 's', ',', 'pobox334', ',', 'stockport', ',', 'sk38xh', ',', 'cost£1.50', '/', 'pm', ',', 'max10mins']\n",
      "label:  1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('text: ', train[i].text)\n",
    "    print('label: ', train[i].label)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Define the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add diagram here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.global_avg_pooling = lambda x: torch.mean(x, dim=-2)\n",
    "        self.fc1 = nn.Linear(embedding_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "EMBEDDING_DIM = 16\n",
    "model = Model(vocab_size, EMBEDDING_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 52119), started 1:41:26 ago. (Use '!kill 52119' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-96fcb66829ceb2ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-96fcb66829ceb2ce\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b967c6471544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d-%H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bag_of_words_embeddings_from_scratch'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./runs/{model_name}/{now}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup logging to Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "MODEL_NAME = 'bag_of_words_embeddings_from_scratch'\n",
    "log_file = f'./runs/{MODEL_NAME}/{now}'\n",
    "writer = SummaryWriter(log_file)\n",
    "\n",
    "# Train lopp\n",
    "from tqdm import tqdm\n",
    "N_EPOCHS = 150\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_size = 0\n",
    "    running_accuracy = 0.0\n",
    "    for batch in tqdm(train_iter):\n",
    "        \n",
    "        # forward pass to compute the batch loss\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "            \n",
    "        # backward pass to update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute train metrics\n",
    "        running_loss += loss.data * x.size(0)\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        running_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        train_size += x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_accuracy = running_accuracy / train_size\n",
    "    \n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    val_size = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_iter:\n",
    "            x = batch.text\n",
    "            y = batch.label.long()\n",
    "            predictions = model(x)\n",
    "            loss = criterion(predictions, y)\n",
    "            \n",
    "            # compute validation metrics\n",
    "            val_loss += loss.data * x.size(0)\n",
    "            _, predicted_classes = torch.max(predictions, 1)\n",
    "            val_accuracy += predicted_classes.eq(y.data).sum().item()           \n",
    "            val_size += x.size(0)\n",
    "            \n",
    "        val_loss /= val_size\n",
    "        val_accuracy /= val_size\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Loss \\t Train: {:.4f} \\t Validation: {:.4f}'.format(epoch_loss, val_loss))\n",
    "        print('Acc: \\t Train: {:.4f} \\t Validation: {:.4f}'.format(epoch_accuracy, val_accuracy))\n",
    "\n",
    "    # log metrics to tensorboard\n",
    "    writer.add_scalars('Loss', {'train': epoch_loss, 'validation': val_loss}, epoch + 1)\n",
    "    writer.add_scalars('Accuracy', {'train': epoch_accuracy, 'validation': val_accuracy}, epoch + 1)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = 0.0\n",
    "test_size = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        # forward pass\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        test_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        test_size += x.size(0)\n",
    "\n",
    "test_accuracy /= test_size\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra. Interact with the model\n",
    "Pay attention how pre-processing and post-processing are necessary to be able to use the model at inference time.\n",
    "\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/issues/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'This is your friend Carl. Come to the Casino and get a discount!',\n",
    "    'This is your friend Carl, do you want to meet later?',\n",
    "    'Would you be interested in buying a car for nothing?',\n",
    "    'Send your card details today and get a prize!',\n",
    "    'I won two tickets to the show, do you want to come with me?',\n",
    "    'I won two tickets to the show, just send an SMS to this number and get them',\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    # pre-process text into integer tokens\n",
    "    model_input = [TEXT.vocab.stoi[token] for token in tokenizer_fn(s)]\n",
    "    # add 0-dimension\n",
    "    model_input = torch.LongTensor(model_input).unsqueeze(0)\n",
    "    \n",
    "    # run model prediction\n",
    "    predictions = model(model_input)\n",
    "    \n",
    "    # post-processing\n",
    "    _, predicted_class = torch.max(predictions, 1)\n",
    "    predicted_class = predicted_class.item()\n",
    "    predicted_class_str = IDX_TO_LABEL[predicted_class]\n",
    "    \n",
    "    # print\n",
    "    print(s)\n",
    "    print(predicted_class_str)\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Visualize the learned word embeddings with the [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if name == 'embed.weight':\n",
    "        embeddings = parameter\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "vocab = TEXT.vocab.itos\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index in [0, 1]:\n",
    "        # skip 0, it's the unknown token.\n",
    "        # skip 1, it's the padding token.\n",
    "        continue\n",
    "        \n",
    "    vec = embeddings[index, :] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files to your local computer (in case you are running this notebook in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
