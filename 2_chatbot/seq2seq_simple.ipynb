{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a chatbot using a *seq2seq* model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible exercises\n",
    "\n",
    "1. Modify code to use LSTM cells instead of GRU.\n",
    "\n",
    "## References\n",
    "- https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "- https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cf54d584af1322e88020549223e907dc/chatbot_tutorial.ipynb#scrollTo=Tlj9jynLIYsH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'formatted_movie_lines.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
      "--\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
      "--\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "def print_lines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "        print('--')\n",
    "\n",
    "print_lines(FILE, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1. Pre-process the dataset and split into `train.csv` and `test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221282/221282 [00:00<00:00, 765419.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original set size:  221282\n",
      "After filtering:   64271\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def load_pairs(file, n=None):\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = []\n",
    "    for line in tqdm(lines):\n",
    "        pair = line.split('\\t')\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "pairs = load_pairs(FILE)\n",
    "print('Original set size: ', len(pairs))\n",
    "\n",
    "import unicodedata\n",
    "def unicode2Ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalize_pairs(pairs):\n",
    "    def normalize_string(s):\n",
    "        s = unicode2Ascii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "        return s\n",
    "    \n",
    "    pairs = [[normalize_string(p[0]), normalize_string(p[1])] for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "def filter_pairs_by_length(pairs, max_length: int = 999999):\n",
    "    \n",
    "    def is_pair_short_enough(p):\n",
    "        return (len(p[0].split(' ')) < max_length) and (len(p[1].split(' ')) < max_length)\n",
    "    \n",
    "    return [pair for pair in pairs if is_pair_short_enough(pair)]\n",
    "\n",
    "pairs = normalize_pairs(pairs)\n",
    "MAX_LENGTH = 10\n",
    "pairs = filter_pairs_by_length(pairs, max_length=MAX_LENGTH)\n",
    "print('After filtering:  ', len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `train.csv` and `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, test_pairs = train_test_split(pairs_df, test_size=0.2, random_state=123)\n",
    "\n",
    "train_pairs.to_csv('train.csv', index=False, header=False)\n",
    "test_pairs.to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2. Load data with torchtext API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_fn(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "sentence_processor = Field(tokenize=tokenizer_fn,\n",
    "                           init_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN,\n",
    "                           batch_first=True)\n",
    "fields = [('src', sentence_processor), ('tgt', sentence_processor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train, test = TabularDataset.splits(\n",
    "    path='',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_processor.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# src_sentence_processor.vocab.stoi['are'] == tgt_sentence_processor.vocab.stoi['are']\n",
    "sentence_processor.vocab.stoi[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataloader`s for train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "from torchtext.data.utils import interleave_keys\n",
    "def sort_key(ex):\n",
    "    # What this does is basically it takes a 16-bit binary representation of lengths and interleaves them.\n",
    "    # Example: lengths len(ex.src)=5 and len(ex.trg)=3 result in f(101, 011)=100111, 7 and 1 in f(111, 001)=101011\n",
    "    # It's basically a heuristic that helps the BucketIterator sort bigger batches first\n",
    "    return interleave_keys(len(ex.src), len(ex.tgt))\n",
    "    \n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train, test),\n",
    "    batch_sizes=(batch_size, batch_size),\n",
    "    device=device,\n",
    "#     sort_key=lambda x: len(x.src), # TODO\n",
    "    sort_key=sort_key,\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 16]\n",
      "\t[.src]:[torch.LongTensor of size 16x10]\n",
      "\t[.tgt]:[torch.LongTensor of size 16x12]\n",
      "tensor([[   2,    7,   68,    6,  131,  187,   73, 1058,    4,    3],\n",
      "        [   2,   46,   21,    6, 1371,   12,  527,    5,    3,    1],\n",
      "        [   2,   50,    8,    9,   75,   45,    5,    3,    1,    1],\n",
      "        [   2,   88,   18,    7,   23,    0,    4,    3,    1,    1],\n",
      "        [   2,    6,   61,    8,    6,  521,   39,    4,    3,    1],\n",
      "        [   2,   44,  120,   24,   14,  348,   18,    4,    3,    1],\n",
      "        [   2,  118,   40,    7,  138,   15,    6,    5,    3,    1],\n",
      "        [   2,    8,   89,  478,   15,   12,  840,    5,    3,    1],\n",
      "        [   2,   42,    6,  530,   69,   12,  199,    5,    3,    1],\n",
      "        [   2,   20,    6, 1146,   32,  524,  105,    5,    3,    1],\n",
      "        [   2,    6,  168,   81,   12,  689, 2999,    5,    3,    1],\n",
      "        [   2,  260,   26,   29,  661,   12, 8317,    5,    3,    1],\n",
      "        [   2,   10,    9,   51,   13,  786, 6535,    4,    3,    1],\n",
      "        [   2,   85,   10,    9,    6,  115,    0,   11,    3,    1],\n",
      "        [   2,   26,  274,   10,   91,   14,   26,    5,    3,    1],\n",
      "        [   2,    6,   29,   27,  313,   21,    6,    5,    3,    1]])\n",
      "tensor([[   2, 2876,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   2,  116, 8783,   21, 4707,   38,   22,   40,   27,  462,    4,    3],\n",
      "        [   2,   16,    9,    8,    7,   41,  160,  159,  198,    6,    4,    3],\n",
      "        [   2,    7,   52,   59,  213,  138,   15,   78,   14, 2656,    4,    3],\n",
      "        [   2,   16,    9,  401,    4,    6, 1285,   69,   47,    5,    3,    1],\n",
      "        [   2,    7,   52,  140,   15,  374,   12,  422,  473,    4,    3,    1],\n",
      "        [   2,  238,    7,   23,   31,   12, 1109,   33,  168,    4,    3,    1],\n",
      "        [   2,   56,   21,   27, 2071, 2549, 8537, 1051, 8880,    4,    3,    1],\n",
      "        [   2,   10,   19,  441,   39,    6,   31,   12, 1976,    4,    3,    1],\n",
      "        [   2,  489,    4,   19,   60,   13, 4167,    4,    4,    4,    3,    1],\n",
      "        [   2,   17,    4,    7,  168,   81,   12,    0,    0,    4,    3,    1],\n",
      "        [   2,   44,   26,    4,    4,    4,   26,    4,    4,    4,    3,    1],\n",
      "        [   2,   16,    9,   77,  134,   33,  230,   81,   10,    4,    3,    1],\n",
      "        [   2,    0,    4,    8,   21,    6,   99,   31,   60,    5,    3,    1],\n",
      "        [   2,   66,    6,   59,  213,   20,   19,   61,   31,    4,    3,    1],\n",
      "        [   2,   17,  281,    7,  108,   59,  313,    4,    6,    5,    3,    1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(train_iter))\n",
    "print(x)\n",
    "print(x.src)\n",
    "print(x.tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3. Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_directions=1, n_layers=1, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        assert n_directions in [1, 2], 'n_directions is either 1 or 2'\n",
    "        self.n_directions = n_directions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout),\n",
    "                          bidirectional=(True if n_directions == 2 else False),\n",
    "                          batch_first=True)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_first=None):\n",
    "        \n",
    "        input_seq = self.embedding(input_seq)\n",
    "        _, hidden_last = self.gru(input_seq, hidden_first)\n",
    "        \n",
    "        # add hidden_state accross 'directions' axis (dim = 1)\n",
    "        h = hidden_last.view(self.n_layers, self.n_directions, -1, self.hidden_dim)       \n",
    "        h = torch.sum(h, dim=1, keepdim=False)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers=1, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        assert embedding_dim == hidden_dim, \"Yes\"\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)       \n",
    "        # unidirectional!\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout),\n",
    "                          batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_step, hidden):\n",
    "        \"\"\"\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        \"\"\"       \n",
    "        # add artificial dimension to match expected format by nn.GRU,\n",
    "        # i.e. (batch_size, seq_len=1, input_size)\n",
    "        input_step = input_step.unsqueeze(1)\n",
    "\n",
    "        embedded = self.embedding(input_step)\n",
    "        # embedded = self.embedding_dropout(embedded)\n",
    "               \n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        # Forward through unidirectional GRU\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # TODO: add in the future.\n",
    "#         # Calculate attention weights from the current GRU output\n",
    "#         attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "#         # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "#         context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        \n",
    "#         # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "#         rnn_output = rnn_output.squeeze(0)\n",
    "#         context = context.squeeze(1)\n",
    "#         concat_input = torch.cat((rnn_output, context), 1)\n",
    "#         concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "#         # Predict next word using Luong eq. 6\n",
    "#         output = self.out(concat_output)\n",
    "\n",
    "        output = self.dropout(self.fc_out(output))\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        # remove artificial dimension we added at the beginning\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim\n",
    "        assert encoder.n_layers == decoder.n_layers\n",
    "        \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        batch_size = tgt.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(src)\n",
    "#         print(hidden.shape)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input_decoder = tgt[:, 0]\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = self.decoder(input_decoder, hidden)\n",
    "           \n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[:, t, :] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input_decoder = tgt[:, t] if teacher_force else top1\n",
    "            \n",
    "#             print('top1: ', top1.shape)\n",
    "#             print('tgt[t]: ', tgt[:, t].shape)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(9852, 256)\n",
       "    (gru): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9852, 256)\n",
       "    (gru): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
       "    (fc_out): Linear(in_features=256, out_features=9852, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(sentence_processor.vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "dropout = 0.20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_dim, n_directions=2, n_layers=n_layers, dropout=dropout)\n",
    "decoder = Decoder(vocab_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test forward pass of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape:  torch.Size([16, 7])\n",
      "tgt shape:  torch.Size([16, 11])\n",
      "output shape:  torch.Size([16, 11, 9852])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "print('src shape: ', batch.src.shape)\n",
    "print('tgt shape: ', batch.tgt.shape)\n",
    "output = model(batch.src, batch.tgt)\n",
    "print('output shape: ', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# crossentropy loss with masking\n",
    "PAD_TOKEN_ID = sentence_processor.vocab.stoi[PAD_TOKEN]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_ID)\n",
    "\n",
    "# Adam as usual\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # output = [batch_size, tgt_len, vocab_size=output_dim]\n",
    "        output = model(src, tgt)\n",
    "                \n",
    "#         #trg = [trg len, batch size]\n",
    "#         #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        # OK!\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # replace this...\n",
    "        # TODO: check why reshape() and not view().\n",
    "        # Maybe first permute() and then view() works?\n",
    "        # output = output[1:].view(-1, output_dim)\n",
    "#         pdb.set_trace()\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        \n",
    "        # replace this...\n",
    "        # tgt = tgt[1:].view(-1)\n",
    "        # TODO: same, why reshape() but not view()\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "#         #trg = [(trg len - 1) * batch size]\n",
    "#         #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        loss = criterion(output, tgt)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-de6c1c434d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     valid_loss = evaluate(model, valid_iterator, criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-9e4d438f4873>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
