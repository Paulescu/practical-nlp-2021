{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of IMDb movie reviews\n",
    "\n",
    "# Bag-of-words model with learned embeddings\n",
    "\n",
    "## Learnings:\n",
    "\n",
    "At the end of this lesson you will know how to:\n",
    "\n",
    "- Create a bag-of-words model for sentiment analysis.\n",
    "\n",
    "- Feed text data into a PyTorch model using `torchtext`\n",
    "\n",
    "- Write a training/validation loop in PyTorch.\n",
    "\n",
    "- Visualize the trained embeddings with the [Embedding Projector]((https://projector.tensorflow.org/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Paulescu/practical-nlp-2021/blob/main/0_sentiment_analysis/0_bag_of_words_with_learned_embeddings.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Data download and pre-processing\n",
    "\n",
    "The original dataset in `http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz` has one file per example, and examples are grouped in folders according to train vs test, and positive vs negative.\n",
    "\n",
    "At the end of `Stage 1: Data pre-processing` we will have data split into 3 CSV files: train.csv, validation.csv, test.csv. In each file, the first column will be the review text, the second column will\n",
    "be the sentiment, 0: negative, 1: positive. This is the format we need the data to be to use `torchtext` data ingestion convenient function `TabularDataset.splits()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw data from disk into Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts:  25000\n",
      "test_texts:  25000\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Auxiliary function to read raw data from disk\n",
    "    into 2 Python lists, one for texts the other for labels\n",
    "    \"\"\"\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in ['pos', 'neg']:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts_, train_labels_ = read_imdb_split('aclImdb/train')\n",
    "print('train_texts: ', len(train_texts_))\n",
    "\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')\n",
    "print('test_texts: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts:  100\n",
      "val_texts:  100\n",
      "test_texts:  100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = \\\n",
    "    train_test_split(train_texts_, train_labels_, test_size=0.2, random_state=1)\n",
    "\n",
    "# train_texts = train_texts[:100]\n",
    "# train_labels = train_labels[:100]\n",
    "# val_texts = val_texts[:100]\n",
    "# val_labels = val_labels[:100]\n",
    "# test_texts = test_texts[:100]\n",
    "# test_labels = test_labels[:100]\n",
    "\n",
    "print('train_texts: ', len(train_texts))\n",
    "print('val_texts: ', len(val_texts))\n",
    "print('test_texts: ', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `train.csv` , `validation.csv`, `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.DataFrame({'text': train_texts, 'label': train_labels},\n",
    "                          columns=['text', 'label'])\n",
    "val_data = pd.DataFrame({'text': val_texts, 'label': val_labels},\n",
    "                        columns=['text', 'label'])\n",
    "test_data = pd.DataFrame({'text': test_texts, 'label': test_labels},\n",
    "                         columns=['text', 'label'])\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Define PyTorch `Dataset` and `DataLoader`s using `torchtext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `Field` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `Dataset` objects for train, validation, test files\n",
    "\n",
    "A `Dataset` is a list of `Example` objects. Each `Example` is a dictionary that maps `Field` names to values.\n",
    "\n",
    "`TabularDataset` provides a convenient way to load columnar data from a csv file into a PyTorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "fields = [('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path='',\n",
    "    train='train_data.csv',\n",
    "    validation='val_data.csv',\n",
    "    test='test_data.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.dataset.TabularDataset'>\n",
      "<class 'torchtext.data.dataset.Dataset'>\n",
      "<class 'torch.utils.data.dataset.Dataset'>\n",
      "<class 'torchtext.data.example.Example'>\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "print(type(train))\n",
    "print(TabularDataset.__bases__[0])\n",
    "print(torchtext.data.dataset.Dataset.__bases__[0])\n",
    "print(type(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access each example data in the same way you access elements of a Python `list`\n",
    "and attributes of a Python `object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  ['there', 'are', 'so', 'many', 'good', 'things', 'to', 'say', 'about', 'this', '“', 'b', '”', 'movie.<br', '/><br', '/>“b', '’', 'maybe', 'in', 'connections', ',', 'but', 'not', 'in', 'commission', '.', 'this', 'is', 'about', 'the', 'best', 'of', 'its', 'genre', 'that', 'i', 'have', 'ever', 'seen', '.', 'a', 'grade', 'a', 'effort', 'by', 'universal', '.', 'the', 'script', 'is', 'well', 'done', ',', 'imaginative', ',', 'and', 'without', 'fault', '.', 'writing', 'credits', ':', 'howard', 'higgin', 'original', 'story', '&', 'douglas', 'hodges', 'story', ',', 'john', 'colton', '(', 'screenplay', ')', '.', 'director', 'lambert', 'hillyer', 'handled', 'the', 'complex', 'story', 'and', 'story', 'locations', 'very', 'well', '.', 'no', 'skimping', 'on', 'the', 'loads', 'of', 'extras', 'and', 'locations', '.', 'i', 'loved', 'beulah', 'bondy', '(', 'jimmy', 'stewarts', 'mother', 'in', '“', 'it', '’s', 'a', 'wonderful', 'life', '”', '.', 'the', 'fem', 'lead', ',', 'frances', 'drake', 'is', 'a', 'beauty', 'and', 'handled', 'her', 'part', 'with', 'grace', 'and', 'pathos', 'for', 'her', 'karloff', 'husband', '.', 'lugosi', 'likewise', 'was', 'correctly', 'underplayed', '.', 'i', 'think', 'this', 'is', 'the', 'best', 'part', 'i', 'remember', 'seeing', 'him', 'in', '.', 'as', 'i', 'said', 'there', 'were', 'so', 'many', 'good', 'things', ':', 'the', 'african', 'discovery', 'of', 'the', 'radium', '“', 'x', '”', ',', 'the', 'melting', 'of', 'the', 'stone', 'statues', '(', '(', 'somewhat', 'reminiscent', 'of', 'the', 'ten', 'little', 'indians', 'in', 'and', 'then', 'there', 'were', 'none', '(', 'agatha', 'christie', ')', '(', 'the', 'barry', 'fitzgerald', 'version', ')', ')', ',', 'the', 'glowing', 'of', 'karlof', 'in', 'the', 'dark', '.', 'karloff', '’s', 'mother', 'played', 'by', 'violet', 'kemble', 'cooper', 'with', 'elegance', '.', 'and', 'because', 'of', 'all', 'these', 'virtues', ',', 'i', 'found', 'myself', 'believing', 'in', 'the', 'science', 'it', 'portrayed', '.', 'i', 'guess', 'that', '’s', 'the', 'mark', 'of', 'a', 'good', 'piece', 'of', 'art', '.']\n",
      "\n",
      "label:  1\n"
     ]
    }
   ],
   "source": [
    "print('text: ', train[0].text)\n",
    "print('\\nlabel: ', train[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary using the training data\n",
    "\n",
    "A `Vocab` object maps each word in the training set into a unique integer.\n",
    "\n",
    "Once you build the vocabulary you can express sentences as sequences of integers.\n",
    "\n",
    "Only training data is used to generate the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1757\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size=10000, min_freq=2)\n",
    "\n",
    "# we will need this later\n",
    "vocab_size = len(TEXT.vocab)\n",
    "print('Vocabulary size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the train/validation/test dataloaders\n",
    "\n",
    "In `torchtext` data loaders are called `BucketIterator`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "    (train, val, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is `train_iter`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n",
      "\n",
      "[torchtext.data.batch.Batch of size 100]\n",
      "\t[.text]:[torch.LongTensor of size 100x1143]\n",
      "\t[.label]:[torch.LongTensor of size 100] \n",
      "\n",
      "text: \n",
      " tensor([[  10,   31,  137,  ...,    0,   18,    0],\n",
      "        [  48,  236,    5,  ...,    1,    1,    1],\n",
      "        [  66,    2,  585,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [1491,    0,    9,  ...,    1,    1,    1],\n",
      "        [ 952,    3,    0,  ...,    1,    1,    1],\n",
      "        [  71,  510,  194,  ...,    1,    1,    1]])\n",
      "label: \n",
      " tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulabartabajo/src/online-courses/practical-nlp-2021/.venv/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# it is an iterator\n",
    "print(type(train_iter))\n",
    "\n",
    "# let's pick the first element and check what it is inside\n",
    "x = next(iter(train_iter))\n",
    "print(x, '\\n')\n",
    "print('text: \\n', x.text)\n",
    "print('label: \\n', x.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Define the neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add diagram here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.global_avg_pooling = lambda x: torch.mean(x, dim=-2)\n",
    "        self.fc1 = nn.Linear(embedding_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.global_avg_pooling(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Model(vocab_size, 16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 46459), started 0:23:18 ago. (Use '!kill 46459' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-741acff123a08556\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-741acff123a08556\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train loss: 0.7096 \t Train accuracy: 0.4600\n",
      "Val loss: 0.7170 \t Val accuracy: 0.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup logging to Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "model_name = 'bag_of_words_embeddings_from_scratch'\n",
    "log_file = f'./runs/{model_name}/{now}'\n",
    "writer = SummaryWriter(log_file)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # train\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_size = 0\n",
    "    running_accuracy = 0.0\n",
    "    for batch in tqdm(train_iter):\n",
    "        \n",
    "        # forward pass to compute the batch loss\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "            \n",
    "        # backward pass to update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # compute train metrics\n",
    "        running_loss += loss.data * x.size(0)\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        running_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        train_size += x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / train_size\n",
    "    writer.add_scalar('training_epoch_loss', epoch_loss, epoch + 1)\n",
    "    epoch_accuracy = running_accuracy / train_size\n",
    "    writer.add_scalar('training_epoch_accuracy', epoch_accuracy, epoch + 1)\n",
    "    \n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    val_size = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            x = batch.text\n",
    "            y = batch.label.long()\n",
    "            predictions = model(x)\n",
    "            loss = criterion(predictions, y)\n",
    "            \n",
    "            # compute validation metrics\n",
    "            val_loss += loss.data * x.size(0)\n",
    "            _, predicted_classes = torch.max(predictions, 1)\n",
    "            val_accuracy += predicted_classes.eq(y.data).sum().item()           \n",
    "            val_size += x.size(0)\n",
    "            \n",
    "        val_loss /= val_size\n",
    "        val_accuracy /= val_size\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Train loss: {:.4f} \\t Train accuracy: {:.4f}'.format(epoch_loss, epoch_accuracy))\n",
    "        print('Val loss: {:.4f} \\t Val accuracy: {:.4f}'.format(val_loss, val_accuracy))\n",
    "        writer.add_scalar('validation_epoch_loss', val_loss, epoch + 1)\n",
    "        writer.add_scalar('validation_epoch_accuracy', val_accuracy, epoch + 1)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 40.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0.0\n",
    "test_size = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_iter:\n",
    "        # forward pass\n",
    "        x = batch.text\n",
    "        y = batch.label.long()\n",
    "        predictions = model(x)        \n",
    "        loss = criterion(predictions, y)\n",
    "\n",
    "        # compute accuracy\n",
    "        _, predicted_classes = torch.max(predictions, 1)\n",
    "        test_accuracy += predicted_classes.eq(y.data).sum().item()\n",
    "        test_size += x.size(0)\n",
    "\n",
    "test_accuracy /= test_size\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Visualize the learned word embeddings with the [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1757, 16])\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if name == 'embed.weight':\n",
    "        embeddings = parameter\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "vocab = TEXT.vocab.itos\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index in [0, 1]:\n",
    "        # skip 0, it's the unknown token.\n",
    "        # skip 1, it's the padding token.\n",
    "        continue\n",
    "        \n",
    "    vec = embeddings[index, :] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files to your local computer (in case you are running this notebook in Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
